import streamlit as st
import uuid
from datetime import datetime
from typing import List

# Import c√°c th∆∞ vi·ªán x·ª≠ l√Ω t√†i li·ªáu v√† k·∫øt n·ªëi CSDL
# Gi·∫£ ƒë·ªãnh c√°c import n√†y ho·∫°t ƒë·ªông ch√≠nh x√°c trong m√¥i tr∆∞·ªùng c·ªßa b·∫°n
try:
    import PyPDF2
    import docx
    from src.database.pinecone_client import upsert_chunk_texts
    from src.database.mongo_client import document_collection
    from src.models.document import Document
    from src.configs.settings import app_config, pinecone_config
except ImportError as e:
    st.error(f"L·ªói import: {e}")
    st.warning("Vui l√≤ng ƒë·∫£m b·∫£o r·∫±ng c·∫•u tr√∫c file v√† c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.")
    st.stop()

# --- C·∫•u h√¨nh trang ---
st.set_page_config(
    page_title="T·∫£i t√†i li·ªáu - THCS Nh√¢n Ch√≠nh",
    page_icon="üìö",
    layout="centered"
)

# --- C√°c h√†m x·ª≠ l√Ω (Gi·ªØ nguy√™n logic g·ªëc) ---

def extract_text_from_pdf(file) -> str:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file PDF."""
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() + "\n"
    return text

def extract_text_from_docx(file) -> str:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file Word."""
    doc = docx.Document(file)
    text = "\n".join([para.text for para in doc.paragraphs])
    return text

def extract_text_from_txt(file) -> str:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file text."""
    content = file.read()
    # Th·ª≠ decode b·∫±ng utf-8, n·∫øu l·ªói th√¨ th·ª≠ latin-1
    try:
        return content.decode('utf-8')
    except UnicodeDecodeError:
        file.seek(0)
        return content.decode('latin-1', errors='ignore')

def extract_text(file, file_type: str) -> str:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n d·ª±a tr√™n lo·∫°i file."""
    extractors = {
        '.pdf': extract_text_from_pdf,
        '.docx': extract_text_from_docx,
        '.txt': extract_text_from_txt,
        '.md': extract_text_from_txt,
    }
    if file_type in extractors:
        return extractors[file_type](file)
    raise ValueError(f"Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_type}")

def split_text_into_chunks(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
    """Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè c√≥ ch·ªìng l·∫•n."""
    if not text or not text.strip():
        return []
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        if end >= len(text):
            break
        start += (chunk_size - overlap)
        
    return [chunk.strip() for chunk in chunks if chunk.strip()]

def process_document(file, filename: str, topic: str) -> tuple:
    """X·ª≠ l√Ω t√†i li·ªáu, tr·∫£ v·ªÅ c√°c ƒëo·∫°n nh·ªè v√† metadata."""
    file_type = '.' + filename.split('.')[-1].lower()
    
    text = extract_text(file, file_type)
    if not text.strip():
        raise ValueError("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ file ho·∫∑c file tr·ªëng.")
        
    chunks = split_text_into_chunks(text)
    if not chunks:
        raise ValueError("N·ªôi dung file qu√° ng·∫Øn ƒë·ªÉ c√≥ th·ªÉ chia nh·ªè.")

    document = Document(
        document_id=str(uuid.uuid4()),
        name=filename,
        topic=topic,
        file_type=file_type,
        file_size=file.tell(),
        chunk_count=len(chunks)
    )
    return chunks, document

@st.cache_data(ttl=60) # Cache k·∫øt qu·∫£ trong 60 gi√¢y
def get_documents_cached():
    """L·∫•y danh s√°ch t√†i li·ªáu t·ª´ CSDL v·ªõi cache."""
    try:
        # Gi·∫£ ƒë·ªãnh h√†m n√†y tr·∫£ v·ªÅ m·ªôt list c√°c dictionary
        return document_collection.get_all_documents()
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ t·∫£i danh s√°ch t√†i li·ªáu: {e}")
        return []

# --- Giao di·ªán ng∆∞·ªùi d√πng ---

st.title("üìö T·∫£i L√™n T√†i Li·ªáu")
st.caption("Th√™m ki·∫øn th·ª©c m·ªõi cho h·ªá th·ªëng AI t·ª´ c√°c file t√†i li·ªáu.")

# 1. Ch·ªçn ch·ªß ƒë·ªÅ
st.subheader("1. Ch·ªçn ch·ªß ƒë·ªÅ cho t√†i li·ªáu")
topic_options = {
    pinecone_config.NAME_SPACE.THONG_TIN_TRUONG.value: "üè´ Th√¥ng tin tr∆∞·ªùng"
}
selected_topic = st.selectbox(
    "Ch·ªß ƒë·ªÅ:",
    options=list(topic_options.keys()),
    format_func=lambda x: topic_options.get(x, "Kh√¥ng x√°c ƒë·ªãnh"),
    label_visibility="collapsed"
)

# 2. T·∫£i file
st.subheader("2. T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n")
uploaded_files = st.file_uploader(
    "H·ªó tr·ª£ PDF, DOCX, TXT, MD. C√≥ th·ªÉ ch·ªçn nhi·ªÅu file.",
    type=['pdf', 'docx', 'txt', 'md'],
    accept_multiple_files=True
)

# 3. X·ª≠ l√Ω
if uploaded_files:
    st.subheader("3. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω")
    
    # Hi·ªÉn th·ªã c√°c file ƒë√£ ch·ªçn trong m·ªôt expander
    with st.expander(f"Xem {len(uploaded_files)} file ƒë√£ ch·ªçn"):
        for file in uploaded_files:
            st.info(f"üìÑ **{file.name}** ({file.size / 1024:.1f} KB)")

    if st.button("üöÄ X·ª≠ l√Ω c√°c t√†i li·ªáu ƒë√£ t·∫£i l√™n", type="primary", use_container_width=True):
        progress_bar = st.progress(0, text="B·∫Øt ƒë·∫ßu qu√° tr√¨nh x·ª≠ l√Ω...")
        success_count = 0
        
        for i, file in enumerate(uploaded_files):
            # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
            progress_text = f"ƒêang x·ª≠ l√Ω file {i+1}/{len(uploaded_files)}: {file.name}"
            progress_bar.progress((i + 1) / len(uploaded_files), text=progress_text)
            
            try:
                # ƒê·∫£m b·∫£o con tr·ªè file ·ªü ƒë·∫ßu
                file.seek(0)
                
                # X·ª≠ l√Ω file
                chunks, document = process_document(file, file.name, selected_topic)
                
                # L∆∞u v√†o CSDL
                # upsert_chunk_texts(chunks, selected_topic, document)
                # document_collection.create_document(
                #     document_id=document.document_id,
                #     name=document.name,
                #     topic=document.topic,
                #     file_type=document.file_type,
                #     file_size=document.file_size,
                #     chunk_count=document.chunk_count
                # )
                
                st.success(f"‚úÖ **{file.name}**: X·ª≠ l√Ω th√†nh c√¥ng ({document.chunk_count} ph·∫ßn).")
                success_count += 1
            
            except Exception as e:
                st.error(f"‚ùå **{file.name}**: ƒê√£ x·∫£y ra l·ªói - {e}")

        # Th√¥ng b√°o k·∫øt qu·∫£ cu·ªëi c√πng
        progress_bar.empty()
        st.info(f"üèÅ **Ho√†n th√†nh!** ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng **{success_count}/{len(uploaded_files)}** t√†i li·ªáu.")
        # X√≥a cache ƒë·ªÉ danh s√°ch ƒë∆∞·ª£c c·∫≠p nh·∫≠t
        st.cache_data.clear()

st.divider()

# Danh s√°ch t√†i li·ªáu hi·ªán c√≥
st.header("üìñ T√†i li·ªáu hi·ªán c√≥ trong h·ªá th·ªëng")

documents = get_documents_cached()

if not documents:
    st.info("Hi·ªán ch∆∞a c√≥ t√†i li·ªáu n√†o trong c∆° s·ªü d·ªØ li·ªáu.")
else:
    # B·ªô l·ªçc
    all_topics = ["T·∫•t c·∫£"] + list(topic_options.keys())
    filter_topic = st.selectbox(
        "L·ªçc theo ch·ªß ƒë·ªÅ:",
        options=all_topics,
        format_func=lambda x: "T·∫•t c·∫£" if x == "T·∫•t c·∫£" else topic_options.get(x, x)
    )

    # L·ªçc v√† hi·ªÉn th·ªã
    if filter_topic == "T·∫•t c·∫£":
        filtered_docs = documents
    else:
        filtered_docs = [doc for doc in documents if doc.get('topic') == filter_topic]
        
    st.write(f"T√¨m th·∫•y {len(filtered_docs)} t√†i li·ªáu.")

    # Hi·ªÉn th·ªã 10 t√†i li·ªáu m·ªõi nh·∫•t
    for doc in reversed(filtered_docs[-10:]):
        with st.container(border=True):
            st.markdown(f"**üìÑ {doc.get('name', 'N/A')}**")
            meta_info = (
                f"**Ch·ªß ƒë·ªÅ:** {topic_options.get(doc.get('topic'), 'N/A')} | "
                f"**K√≠ch th∆∞·ªõc:** {doc.get('file_size', 0) / 1024:.1f} KB | "
                f"**S·ªë ph·∫ßn:** {doc.get('chunk_count', 0)}"
            )
            st.caption(meta_info)